{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c22969b480199c1fe1fcf04f1f4713213b52b49"
   },
   "source": [
    "Introduction: Time series are chronologically ordered sequences of data. There are four components to it: level, trend, seasonality, and noise. The level represents the average value in the series, while the trend shows the direction in which it is moving (upward, downward, or constant). Any patterns that occur during certain seasons or periods can be attributed to seasonality. As a final definition, noise refers to random factors such as sampling variability or external influences such as politics, natural disasters, or strikes. \n",
    "Considering that a time series can be stationary, meaning it exhibits no trend or seasonality, both the trend and seasonality components are optional. \n",
    "Stationarity vs Stationarity: A time series that is stationary has a constant trend and seasonality component. Seasonality, on the other hand, refers to periodic fluctuations in a series. In order to avoid misleading relationships, trend and stationarity need to be removed from a time series. As a result, any observed patterns or correlations are not simply due to these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "\n",
    "# Read data and make Date column index\n",
    "djia = pd.read_csv(\"../input/DJIA_table.csv\", parse_dates=['Date'], index_col='Date')\n",
    "print(djia.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet is written in Python and uses several libraries. Firstly, it imports the necessary libraries for data processing and visualization: numpy, pandas, and matplotlib.pyplot. The libraries provide functions and tools for mathematical operations, array manipulation, data processing, and CSV file input and output. \n",
    "A CSV file named \"DJIA_table.csv\" is then read by the code. \"../input/DJIA_table.csv\" is the path to the file. The `pd.read_csv` function from the pandas library is used to read the CSV file into the pandas DataFrame. 'Date' is parsed as a date, while 'index_col='Date'` sets the 'Date' column as the index column of the DataFrame. \n",
    "The resulting DataFrame, named `djia`, contains the data extracted from the CSV file. Rows and columns represent observations and variables, respectively, in a tabular format. With the 'Date' column serving as an index, time-based operations and analysis are made easy. \n",
    "In order to verify that the data has been loaded correctly, the code prints the first few rows of the Djia DataFrame using the Head() function. Displays the top records of the DataFrame, giving a glimpse of the data and confirming its loading. \n",
    "This code segment sets up the necessary libraries, reads data from a CSV file, and stores it in a pandas DataFrame. The purpose of this step is to prepare the data for further processing, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "ts = djia['Open']\n",
    "ts = ts.head(100)\n",
    "plt.plot(ts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a subset of data from the DJIA DataFrame, this code snippet generates a plot using the Matplotlib.Pyplot library in Python. \n",
    "From the djia DataFrame, the code selects a column called 'Open', which represents the opening prices of financial instruments and indices. Assign this column to the variable `ts`, which stands for \"time series\". In essence, `ts` now contains the sequence of opening prices. \n",
    "A code in the following section selects the first 100 data points in the TS series. By doing so, we create a new subset of data that includes the opening prices for the first 100 time periods. \n",
    "This line plot was generated using the plt.plot() function from the matplotlib.pyplot library. An output line graph is generated from the ts series input by this function. Plotting the opening prices over the specified period provides a visual representation of the trend and pattern. \n",
    "The code segment extracts the 'Open' column from the Djia DataFrame, selects the first 100 points, and creates a line plot showing the opening prices over time. By doing so, you can visually analyze the price trends and identify any patterns or fluctuations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07745d5a63d8de51e4f2776c83df83cc6de15391"
   },
   "source": [
    "We will discuss a method for testing a time series' stationarity in this section. This section contains code sourced from [here](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/). \n",
    "Our focus will be on the Augmented Dickey-Fuller test. An autoregressive model with a unit root is assumed to indicate non-stationarity in this test. In contrast, if the test rejects the null hypothesis, then the time series is stationary. Performing this test is crucial to avoiding spurious regressions and ensuring that the model accurately captures the underlying patterns. \n",
    "Using a graph that shows the variation of mean and standard deviation over time, we can determine whether a time series has unit roots. A slight variation in the standard deviation is sufficient to reject the null hypothesis. \n",
    "A p-value is provided by the Augmented Dickey-Fuller test. When the calculated p-value falls below certain critical values, we cannot reject the null hypothesis, indicating that the model has a unit root and is non-stationary. We can reject the null hypothesis if the p-value exceeds these critical values, indicating that the model does not have a unit root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf25791fbd4c641f9086cac08594fcf40569418c"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(tieseries):\n",
    "\n",
    "    rollmean = tieseries.rolling(window=12).mean()\n",
    "    rollstd = tieseries.rolling(window=12).std()\n",
    "\n",
    "    plt.plot(tieseries, color=\"blue\", label=\"Original\")\n",
    "    plt.plot(rollmean, color=\"red\", label=\"Rolling Mean\")\n",
    "    plt.plot(rollstd, color=\"black\", label=\"Rolling Std\")    \n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(tieseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput,'%f' % (1/10**8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, a function called test_stationarity is defined in the statsmodels.tsa.stattools module. This function tests a time series for stationarity. \n",
    "Two rolling statistics are calculated within the function: the rolling mean and the rolling standard deviation. On the input time series, moving windows of size 12 are used to compute these statistics. As the rolling mean measures the average value of the time series within the window, the rolling standard deviation measures its dispersion or variability. \n",
    "The code then creates a plot with three lines: the original time series in blue, the rolling mean in red, and the rolling standard deviation in black. Using this plot, you can visualize how the mean and standard deviation of the time series change over time. Trends and seasonality can be discerned through it. \n",
    "In the next step, the Dickey-Fuller test is performed, which is a statistical test used to determine the presence of unit roots and the stationarity of time series. Test results are displayed on the console. Test statistic, p-value, number of lags, and number of observations are displayed. In addition, critical values are calculated and displayed to compare with the test statistic and determine significance. \n",
    "A summary of this code can be summarized as follows: it calculates rolling statistics, generates a plot to visualize them, and performs a Dickey-Fuller test to determine whether a given time series is stationary. It provides both visual and statistical information to determine whether a time series is stationary or non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "024660eee7571485564780619ec531a830e3b544"
   },
   "outputs": [],
   "source": [
    "test_stationarity(ts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function call called test_stationarity(ts) tests the stationarity of a given time series. \n",
    "Test_stationarity take a time series argument ts and performs a series of steps to assess its stationarity. \n",
    "\n",
    "Using a 12-window size, it calculates the rolling mean and rolling standard deviation of the time series. A rolling mean represents the average value of a time series within a moving window, while a rolling standard deviation measures its variability. \n",
    "In the next step, the function generates a plot that displays the original time series in blue, the rolling mean in red, and the rolling standard deviation in black. The plot illustrates how the mean and standard deviation change over time, allowing an assessment of any trends or fluctuations. \n",
    "The function performs the Dickey-Fuller test after generating the plot. A time series can be tested for unit roots and stationarity using this statistical test. On the console, the test statistic, p-value, number of lags, and number of observations are printed. \n",
    "The function test_stationarity(ts) applies a set of operations to analyze the stationarity of a time series. Using rolling statistics, a plot is generated to visualize them, and the Dickey-Fuller test is performed to determine whether the time series is stationary. To determine whether a time series is stationary or non-stationary, the function provides both visual and statistical insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f4d15d7f8f75ec214ece53a819bdcfd46347223"
   },
   "source": [
    "We will discuss trend in a time series in this section. It determines whether the series is trending upwards or downwards and directly affects it. In order to understand the direction of the series, it is crucial to understand the trend. Making predictions requires removing the trend component in order to ensure stationarity. \n",
    "Time series trends are estimated and removed in the section titled \"Dealing with trends.\". Linear regression, moving averages, and decomposition are some of these methods. To determine which technique produces the most desirable results, these techniques will be evaluated and compared. \n",
    "A subsection titled \"Estimating trend\" discusses the importance of estimating the trend component to better understand time series behavior. Forecasting, however, does not need the trend component since it represents a non-stationary series. As a result, the focus shifts to comparing different methods and selecting the most effective one. In addition, log transformation is mentioned as a means of simplifying the analysis. \n",
    "The purpose of this section is to emphasize the significance of the trend component in a time series and its impact on forecasting. For simplicity, it highlights the option of log transformation for estimating and removing trends. In time series analysis, the goal is to improve understanding of trends and find the most suitable approach to estimating trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3f61b021a62fe716d1093620b85d229cbbe2567"
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "ts_log = np.log(ts)\n",
    "fig = plt.figure(constrained_layout = True)\n",
    "gs_1 = gridspec.GridSpec(2, 3, figure = fig)\n",
    "ax_1 = fig.add_subplot(gs_1[0, :])\n",
    "ax_1.plot(ts_log)\n",
    "ax_1.set_xlabel('time')\n",
    "ax_1.set_ylabel('data')\n",
    "plt.title('Logged time serie')\n",
    "\n",
    "ax_2 = fig.add_subplot(gs_1[1, :])\n",
    "ax_2.plot(ts)\n",
    "ax_1.set_xlabel('time')\n",
    "ax_1.set_ylabel('data')\n",
    "plt.title('Original time serie')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet uses the `matplotlib` library in Python to create a figure with two subplots representing different time series. \n",
    "First, the `matplotlib.gridspec` module is imported to create a customized grid layout for the subplots. \n",
    "We transform a given time series, ts, by taking its natural logarithm. The transformed time series is stored in the variable ts_log. For data with exponential growth or large variations, this logarithmic transformation is commonly used to simplify the analysis. \n",
    "The figure object is created using plt.figure(constrained_layout=True), which ensures that the subplots are properly arranged. \n",
    "In order to define the grid layout for the subplots, use gridspec.GridSpec(2, 3, figure=fig), which specifies a 2x3 grid layout with the created figure. \n",
    "Using the function fig.add_subplot(gs_1[0, :]), the first subplot, ax_1, is added to the figure. A plot of the logarithmically transformed time series (ts_log) is shown in this subplot. The x-axis and y-axis labels can be set using `ax_1.set_xlabel('time')` and `ax_1.set_ylabel('data')`, respectively. 'Logged time series' is the title of the subplot. \n",
    "In the same manner, the second subplot, represented by `ax_2`, is added to the figure using `fig.add_subplot(gs_1[1, :])`. The subplot shows the original time series (`ts`). Labels for the x-axis and y-axis are set, and the title of the subplot is 'Original time series'. \n",
    "The code creates a figure with two subplots. In the first subplot, the logarithmically transformed time series is plotted, while in the second subplot, the original time series is plotted. Matplotlib.gridspec is used to customize the figure layout. Analyzing the patterns and characteristics of the logged and original time series can be accomplished using the resulting plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f9d3472f4416ffa289035b576d90ab6ea26fd5e"
   },
   "source": [
    "#### Linear regression #### This will give us a linear trend estimation. Python implementation requires importing sklearn. Separate X and Y axes and add a second dimension to Y axis for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d72164a7c5664e77603951e153562711fd58012c"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "ts_wi = ts_log.reset_index()\n",
    "df_values = ts_wi.values\n",
    "train_y = df_values[:,1]\n",
    "train_y = train_y[:, np.newaxis]\n",
    "train_x = ts_wi.index\n",
    "train_x = train_x[:, np.newaxis]\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train_x, train_y)\n",
    "pred = regr.predict(train_x)\n",
    "plt.plot(ts_wi.Date, pred)\n",
    "plt.plot(ts_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's sklearn library, this code snippet performs linear regression on a time series. \n",
    "In the first step, the code imports the necessary modules from Sklearn, including datasets and linear models. A linear regression analysis can be performed using these modules, which provide functions and tools for working with datasets. \n",
    "The `ts_log` time series is reset to include the index separately. New variable ts_wi is created for this modified time series. \n",
    "DataFrame values are extracted and stored in a variable named df_values. The purpose of this step is to prepare the data for regression analysis. \n",
    "A target variable, has been extracted from the df_values array, which represents the column of data whose value we are trying to predict. A new axis is created using `np.newaxis` to maintain the required shape for regression analysis. \n",
    "Features (`train_x`) are derived from the index of the `ts_wi` DataFrame. The data is reshape to the appropriate format, similarly to train_y. \n",
    "The linear regression model is initialized with the linear_model.LinearRegression() method. \n",
    "Following this, the regression model is fitted using the \"fit()\" function using the input features (\"train_x\") and the target variable (\"train_y\") as inputs. \n",
    "A prediction is generated by using the predict() function using the trained regression model and the input features (train_x). \n",
    "A plot is created using `plt.plot()` to display the predictions (`pred`) against the date values from the `ts_wi.Date` column. The fitted regression line is shown in this plot. \n",
    "In addition, the original log-transformed time series (`ts_log`) is plotted using `plt.plot()` to compare it with the predicted values. \n",
    "This code segment applies linear regression to a time series. By using the index as input features and a specific column of data as target variables, a regression model is trained. Using the trained model, predictions are generated, which are then plotted against the date values. Additionally, the code plots the original log-transformed time series for comparison. Plots of the fitted regression line and its fit to the actual data are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bfbaa2f5698987ff911d2c63d81a7abb99c0c4e"
   },
   "source": [
    "#### Moving Average #### Moving averages can also be used to estimate trends. By smoothing the series, just the representative data was obtained. \n",
    "Based on repeated observation, it was determined that 12 gave better results in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1de91eefc750dff5f9f5419dee3375be37b156d"
   },
   "outputs": [],
   "source": [
    "mov_average = ts_log.rolling(12).mean()\n",
    "plt.plot(mov_average)\n",
    "plt.plot(ts_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code snippet that calculates moving averages and generates plots using the Matplotlib library. \n",
    "A moving average is calculated for the logarithmically transformed time series (`ts_log`). At each time point, the moving average is calculated using a window size of 12, which means it averages the preceding 12 data points. Moving averages are assigned to a variable named `mov_average`. \n",
    "Moving average series are displayed through a plot created through plt.plot(). Smoothing out short-term fluctuations and emphasizing longer-term patterns, this plot shows the trend of the time series. \n",
    "Furthermore, the original log-transformed time series (`ts_log`) is also plotted using `plt.plot()`. A comparison can be made between the original time series and the moving average series using this plot. \n",
    "This code calculates the moving average of a logarithmically transformed time series and plots both the moving average and the original series. By removing short-term noise and highlighting the underlying patterns over a specified window size, the plot illustrates the time series trend."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a75b867fe4ba777621184efb586765f3d8b7192"
   },
   "source": [
    "Time series forecasting is discussed in the subsection titled \"Eliminating trend.\". It is essential to achieve stationarity for regression analysis to work effectively with time series data. \n",
    "As discussed earlier, a method for trend estimation is used to eliminate the trend. In order to achieve stationarity, the estimated trend component is subtracted from the time series. \n",
    "A moving average method is used to eliminate the trend in this case. The code snippet does not mention the specific results obtained from applying this method. \n",
    "In the Dickey-Fuller test, a statistical test used to assess stationarity, the data exhibits values below the critical value of 5%. Based on this, there is a 90% level of confidence that the data is stationary, thus satisfying the regression analysis requirement. \n",
    "This subsection emphasizes the importance of eliminating the trend component from time series forecasting. In particular, the moving average approach is discussed as a method of trend estimation and subtraction. Based on the Dickey-Fuller test, the data demonstrate stationarity, making it suitable for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25f4de6227759e7d6e7a9362cfbc3f2bd0d65881",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_log_mov_av_diff = ts_log - mov_average\n",
    "#ts_log_mov_av_diff.head(12)\n",
    "ts_log_mov_av_diff.dropna(inplace=True)\n",
    "\n",
    "test_stationarity(ts_log_mov_av_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze a time series' stationarity, this code snippet performs additional operations. \n",
    "A new time series called `ts_log_mov_av_diff` is created by subtracting the moving average series (`mov_average`) from the logarithmically transformed time series (`ts_log`). Time series are subtracted to eliminate trend components. \n",
    "Using the dropna() function, we remove any missing or NaN (Not a Number) values from the ts_log_mov_av_diff series. To ensure the integrity of the data and to avoid potential issues in subsequent analyses, this step is necessary. \n",
    "Lastly, the test_stationarity() function is called with the input of the log_mov_av_diff series. According to the previous explanation, this function tests the stationarity of the transformed time series. The program calculates rolling statistics, displays them in a plot, and assesses the stationarity of the time series by using the Dickey-Fuller test. This analysis provides insight into the stationarity properties of the `ts_log_mov_av_diff` series. \n",
    "By subtracting the moving average from the logarithmically transformed time series, this code segment generates a new time series. The transformed series is then tested for stationarity and any missing values are removed. As a result, stationarity characteristics can be better understood and trend elimination can be more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14d0386a627237368cdab78d790856dfe572f124"
   },
   "source": [
    "On the other hand, we have a linear regression with a value over 10%, which means we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "ad9827640e70aa658d106fe79b151ad70b28d09b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_log_mov_reg_diff = ts_log - pred[:,0]\n",
    "#ts_log_mov_av_diff.head(12)\n",
    "ts_log_mov_reg_diff.dropna(inplace=True)\n",
    "\n",
    "test_stationarity(ts_log_mov_reg_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying linear regression, this code snippet analyzes a time series' stationarity. \n",
    "First, a new time series called `ts_log_mov_reg_diff` is created by subtracting the predicted values (`pred[:,0]`) obtained from the linear regression model from the logarithmically transformed time series (`ts_log`). In this subtraction, the linear regression model estimates and predicts the trend component. \n",
    "A NaN (Not a Number) value will also be removed from the ts_log_mov_reg_diff series using the dropna() function. As a result of this step, data integrity is ensured and the series is prepared for further analysis. \n",
    "Finally, the `test_stationarity()` function is called with the `ts_log_mov_reg_diff` series as a parameter. As discussed earlier, this function tests the stationarity of transformed time series. A rolling statistic is calculated, a plot is generated, and a Dickey-Fuller test is conducted to determine whether the time series is stationary. Based on the results obtained, we can gain insight into the stationarity properties of the ts_log_mov_reg_diff series. \n",
    "By subtracting the predicted values from the logarithmically transformed time series, this code segment generates a new time series. The transformed series is tested for stationarity and missing values are removed. As a result, we are able to gain a deeper understanding of the stationarity characteristics after accounting for the linear regression-estimated trend. By analyzing the data, we are able to determine whether the trend removal process has been effective and if the resulting series is stationar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3bbb2920eaf2a5a53336f50acbc03e687cfad341"
   },
   "source": [
    "Finally, we have differencing, which involves subtracting a previous instance of time from the original. This allows us to calculate the difference between consecutive time series values. \n",
    "\n",
    "It is also possible to increase the order of differencing by taking the previous point of the first difference, and the order can be further increased by repeating the process. \n",
    "\n",
    "When the lag-1 term in the autocorrelation function (ACF) is zero or negative, or if the autocorrelations are small or lack significant values, a higher order of differencing is not required. \n",
    "By identifying the point at which the standard deviation is lowest, you can also determine the optimal order of differencing. At this level, the greatest reduction in variability can be achieved through differencing. \n",
    "A time series can be differentiated in order to remove trends and achieve stationarity. We can create differences between consecutive values by subtracting previous instances of the series. Observing the standard deviation determines the optimal order to increase the order of differencing based on certain rules. In these steps, the time series is prepared for further analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a659966769b00bc4a1a5afb9e5206e348c2a8ad5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics import tsaplots as tsa\n",
    "ts_log_diff = ts_log - ts_log.shift(1)\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = tsa.plot_acf(ts_log_diff.iloc[13:], lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = tsa.plot_pacf(ts_log_diff.iloc[13:], lags=40, ax=ax2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a differenced time series, this code snippet uses the statisticsmodels.graphics.tsaplots module in Python. \n",
    "First, a new time series called `ts_log_diff` is created by subtracting the previous value of the logarithmically transformed time series (`ts_log`) from the current value using the `shift()` function. In this operation, the difference between consecutive values of a time series is calculated. \n",
    "A figure object with a size of 12x8 is created using plt.figure(figsize=(12,8)). Autocorrelation and partial autocorrelation plots are shown in this figure. \n",
    "An Axes object (`ax1`) is added to the figure using `fig.add_subplot(211)`. The autocorrelation plot will be contained in this Axes object. \n",
    "The autocorrelation plot is generated using the tsa.plot_acf() function. It takes the differenced time series (`ts_log_diff`) as input and specifies the number of lags to consider using the `lags` parameter. Plots are assigned to the `fig` variable. \n",
    "Another Axes object (`ax2`) is added to the figure using `fig.add_subplot(212)`. The partial autocorrelation plot will be displayed in this Axes object. \n",
    "Using tsa.plot_pacf(), a partial autocorrelation plot can be generated. It takes the differenced time series (`ts_log_diff`) as input and specifies the number of lags to consider using the `lags` parameter. Plots are assigned to the `fig` variable. \n",
    "By subtracting the previous value from the current value, this code segment calculates the differenced time series. Afterwards, two plots are generated: an autocorrelation plot and a partial autocorrelation plot. For time series analysis and modeling, these plots provide insights into autocorrelation and partial autocorrelation patterns of the differenced time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "024f2032fddb5bb571907e0394147a35c1f360b1"
   },
   "source": [
    "Validate the differencing by performing a stationary test once we have chosen the differencing that best fits our data. We are 99% confident that the time series is stationary since the parameter related to stationarity, i.e. test statistic, is less than 1% critical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56493b38211565aa250126e3263ad47906d44f45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_log_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet analyzes the stationarity of a differenced time series by performing additional operations. \n",
    "In the first step, missing values from the differenced time series are removed using the dropna() function. In this step, the data is complete and ready for further analysis. \n",
    "Next, the `test_stationarity()` function is called with the differenced time series (`ts_log_diff`) as the input. As discussed earlier, this function tests the stationarity of transformed time series. Rolling statistics are calculated, a plot is generated to visualize these statistics, and the Dickey-Fuller test is conducted to assess stationarity. Based on the results obtained, we can gain insight into the stationarity properties of the `ts_log_diff` series. \n",
    "By removing any missing values, this code segment prepares the differenced time series. The transformed series is then tested for stationarity, allowing for a deeper understanding of its stationarity characteristics. In addition to determining whether the differencing process achieves stationarity, this analysis provides useful information for forecasting and modeling time series."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "455c41358ceb9cc71778b9fb99494e144d35ac95"
   },
   "source": [
    "Seasonality and trend."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same approach as the previous section, we will address trend and seasonality components in a time series. Decomposition is the process of removing these components. \n",
    "Decomposition is a method used to remove both trend and seasonal components from time series, similar to the process of removing trend. By doing so, we are able to separate the time series into its constituent parts. \n",
    "We will not differenciate the time series in this section to facilitate practicality and better visualization. In other words, all components of the time series, including trend and seasonality, will be represented in the graphics and plots generated. \n",
    "The purpose of this section is to discuss the application of decomposition to address trend and seasonality. Using this technique, we can remove these components and analyze the remaining components more effectively. In this case, the time series are not differentiated in order to provide practicality and comprehensive visual representation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c430b8d7c71f72a994f1b4c722db86183a83233"
   },
   "source": [
    "The decomposition process. To make a time series stationary, seasonal decompose is the fastest way to remove trend and seasonality components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6d313601b3938f3d63bb3736905cda7c873c4cb"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(ts_log, freq=4, model='additive')\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_log, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below uses the statisticsmodels.tsa.seasonal module in Python to perform seasonal decomposition. \n",
    "First, the `seasonal_decompose` function is used to decompose the logarithmically transformed time series (`ts_log`). It is set to 4, which indicates the frequency of the seasonal pattern in the data. In this case, the model parameter is set to 'additive', which indicates that an additive model will be used to perform the seasonal decomposition. \n",
    "Three components are generated by the decomposition process: trend, seasonality, and residuality. \n",
    "Long-term, non-seasonal patterns are represented by the trend component. As a result, it captures the overall direction and persistence of the data. \n",
    "Seasonal patterns in the time series are captured by the seasonal component. An oscillation occurs at a specific frequency on a regular basis. \n",
    "Unlike the trend and seasonal components, the residual component represents random and unpredictable variations in the time series. \n",
    "To visualize the original time series, trend component, seasonal component, and residual component, a series of subplots are created using plt.subplot() and plt.tight_layout(). \n",
    "In the first subplot (`plt.subplot(411)`), the original time series (`ts_log`) is plotted with the label 'Original'. \n",
    "'Trend' is plotted with the label in the second subplot (plt.subplot(412)). \n",
    "This subplot in the plot determines the season with the label 'Seasonality'. \n",
    "In the fourth subplot (414), the residual component is shown with the label 'Residuals'. \n",
    "For each subplot, plot.legend(loc='best') statements are used, while plot.tight_layout() ensures the plots are arranged and spaced properly. \n",
    "The purpose of this code segment is to perform seasonal decomposition of a time series. Time series are separated into trend, seasonal, and residual components. In order to visualize the patterns and contributions to the overall time series, the resulting components are plotted in individual subplots. Time series can be decomposed in this way to gain a deeper understanding of the underlying components and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bb2c1651d52dadb1b0918afd0b156c9143c6c25"
   },
   "source": [
    "By comparing the test statistic value to the critical value of 1%, we have 99% confidence that the residual of the series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ddd35c00278c342bae1d1034ba1cf4b81238ab7"
   },
   "outputs": [],
   "source": [
    "#ts_decompose = residual\n",
    "ts_decompose = ts_log_diff\n",
    "ts_decompose.dropna(inplace=True)\n",
    "test_stationarity(ts_decompose)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seasonal decomposition, this code snippet performs additional operations on the time series. \n",
    "The first step is to assign the residual component of the seasonal decomposition, represented by `residual`, to a new time series called `ts_decompose`. Alternatively, the commented line assigns the differenced time series, represented by `ts_log_diff`, to `ts_decompose`. \n",
    "The `dropna()` function removes any missing or NaN (Not a Number) values from ts_decompose. In this way, the data is complete and ready for further analysis. \n",
    "Following this, the output of the simulation is passed to the test_stationarity() function. As discussed earlier, this function tests the stationarity of the transformed time series. The program calculates rolling statistics, generates a plot to visualize these statistics, and performs the Dickey-Fuller test for stationarity. Using the results obtained, we can gain insight into the stationarity properties of the `ts_decompose` series. \n",
    "As a summary, this code segment assigns the residual component or differenced series to a new series after seasonal decomposition. The transformed series is tested for stationarity and missing values are removed. After considering seasonal decomposition, this analysis helps determine the stationarity characteristics and provides valuable information for further forecasting and modeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46dbd88666a0b07a9dfcac616dc0d0961b9951aa"
   },
   "source": [
    "Time series forecasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series forecasting involves making predictions based on stationary series. Time series forecasting commonly uses the ARIMA (AutoRegressive Integrated Moving Average) model. It is true that ARIMA models are primarily designed for non-seasonal data, but they are also capable of handling seasonal data. \n",
    "In order to choose the best ARIMA model, the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots must be analyzed. Data correlations and lags are revealed in these plots. \n",
    "ACF and PACF plots should also be analyzed according to certain rules. \n",
    "\n",
    "According to the rules: If the PACF plot exhibits a sharp cutoff and/or the lag-1 autocorrelation is positive, it suggests adding one or more AutoRegressive (AR) terms. In other words, the previous values of the series can help predict the future values. \n",
    "- If the ACF plot shows a sharp cutoff and/or the lag-1 autocorrelation is negative, then one or more Moving Average (MA) terms should be considered. In other words, the previous errors or residuals of the series can be used to predict future values. \n",
    "The ACF and PACF plots are used to select the appropriate ARIMA model for forecasting time series. The plots help identify the presence of autocorrelation and guide the inclusion of AR and MA terms. For interpreting ACF and PACF plots and determining the appropriate ARIMA terms, these rules provide guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c95666cae0296bbc4b890a5812ea37317ac4c93",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = tsa.plot_acf(ts_decompose, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = tsa.plot_pacf(ts_decompose, lags=40, ax=ax2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a decomposed time series, this code snippet generates autocorrelation function and partial autocorrelation function plots. \n",
    "Using plt.figure(figsize=(12,8)), a figure object with a size of 12x8 is created. The ACF and PACF plots will be displayed in this figure. \n",
    "An Axes object (`ax1`) is added to the figure using `fig.add_subplot(211)`. To plot the ACF, this Axes object represents the first subplot. \n",
    "To generate the ACF plot, the tsa.plot_acf() function is used. A time series using the ts_decompose function is passed as input, and the number of lags to consider is set to 40. Plots are assigned to the `fig` variable. \n",
    "A second Axes object (`ax2`) is added to the figure using `fig.add_subplot(212)`. The PACF will be plotted using this Axes object. \n",
    "PACF plots are generated using the tsa.plot_pacf() function. ACF plots use the same inputs as ACF plots, but they depend on the time series input being passed to the function. The number of lags is set using the `lags` parameter. Plots are assigned to the `fig` variable. \n",
    "The code segment displays the ACF and PACF plots in a figure. The first subplot generates the ACF plot, and the second subplot generates the PACF plot. PACF plots represent the correlation between a time series and its lagged values after removing the correlation explained by previous lags, while ACF plots represent the correlation between a time series and its lagged values. Plots like these provide insights into the potential AR and MA terms for ARIMA model selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2657801fb605a18b5f01c2ba377332cdf23d2d94"
   },
   "source": [
    "Models based on ARIMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA models can be divided into nonseasonal and seasonal models. The nonseasonal ARIMA model is denoted as $ARIMA(p,d,q)$, where:\n",
    "- $p$ represents the number of autoregressive terms, which capture the influence of previous observations on the current observation.\n",
    "- $d$ represents the number of nonseasonal differences required to make the time series stationary.\n",
    "- $q$ represents the number of lagged forecast errors, also known as moving average terms, included in the prediction equation.\n",
    "\n",
    "In simple terms, the forecast equation for ARIMA models is an equation that estimates the value of the time series at a given point. It can be written as follows:\n",
    "\"The estimated value $\\hat{y}_t$ at time $t$ is equal to a constant term $\\mu$ plus the sum of the products of autoregressive terms ($\\phi_1 y_{t-1}$, $\\phi_2 y_{t-2}$, ..., $\\phi_p y_{t-p}$) and the negative sum of the products of moving average terms ($\\theta_1 e_{t-1}$, $\\theta_2 e_{t-2}$, ..., $\\theta_q e_{t-q}$), where $y_t$ represents the observations, $e_t$ represents the forecast errors, and $\\phi$ and $\\theta$ are coefficients.\"\n",
    "\n",
    "When determining the appropriate AR and MA terms for the ARIMA model, one approach is to examine the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots. By analyzing these plots, the number of AR terms can be determined by identifying at which lag the PACF plot cuts off significantly. Similarly, the number of MA terms can be determined by identifying significant lags in the ACF plot.\n",
    "\n",
    "After conducting various tests and analyses, it was found that the best results for the ARIMA model were obtained with $p = 1$, $d = 0$, and $q = 0$. This indicates that only one autoregressive term was needed, and there were no nonseasonal differences or moving average terms in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de236dfda93e06d86e02c5a00bdbccf46e504600",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Raw time serie\n",
    "fig = plt.figure(constrained_layout=True) \n",
    "gs = gridspec.GridSpec(2, 1, figure=fig)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ts_decompose)\n",
    "ax.set_xlabel('time [months]')\n",
    "ax.set_ylabel('data')\n",
    "ax.set_title('Logged data')\n",
    "\n",
    "model = ARIMA(ts_decompose, order=(1, 0, 0))\n",
    "res = model.fit(disp=-2)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, :])\n",
    "ax2.plot(res.fittedvalues)\n",
    "ax2.set_xlabel('time [months]')\n",
    "ax2.set_ylabel('data')\n",
    "ax2.set_title('ARIMA model')\n",
    "\n",
    "print('ARIMA RMSE: %.6f'% np.sqrt(sum((res.fittedvalues-ts_decompose)**2)/len(ts)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ARIMA (AutoRegressive Integrated Moving Average) model is applied to a decomposed time series in this code snippet. \n",
    "The first step is to import the necessary modules from the statsmodels.tsa library, ARIMA and SARIMAX. \n",
    "It is created a figure object to display the raw time series and the fitted values of the ARIMA model. Using constrained_layout and gridspec.GridSpec(2, 1), the figure is divided into two subplots. \n",
    "There are two subplots: ax and ts_decompose (ax and ts_decompose respectively). Data values are represented on the y-axis, while time is represented on the x-axis. Labels and titles are provided for the plot. \n",
    "In order to create an ARIMA model, the order parameter must be set to (1, 0, 0). The model has one autoregressive term, no differencing term, and no moving average term. \n",
    "A fitted ARIMA model is then applied to the ts_decompose series using the fit method. The `disp` parameter is set to `-2` in order to suppress convergence-related messages during the fitting process. \n",
    "In the second subplot (`ax2`), the ARIMA model fitted values are shown. Time is represented on the x-axis, and data values are displayed on the y-axis. Accordingly, the plot is labeled and titled. \n",
    "This code calculates and prints the root mean squared error (RMSE) between the fitted values and the original `ts_decompose` series. A model's RMSE measures its accuracy in fitting the data. \n",
    "The code segment creates two subplots to visualize the decomposed time series and the ARIMA model fitted values. The decomposed series is fitted with an ARIMA model with specified order parameters and the fitted values are plotted. To evaluate the model's performance, it calculates and prints the RMSE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a1d93b5761e1889acb7b1d40403ae4dbd525655"
   },
   "source": [
    "In Section 5.2, we introduce the SARIMAX (Seasonal AutoRegressive Integrated Moving Average with Exogenous Factors) model, which incorporates seasonality into the ARIMA model. \n",
    "As with the ARIMA model, the nonseasonal components of the SARIMAX model are represented by the parameters $p$, $d$, and $q$. A moving average, a differencing term, and an autoregressive term are controlled by these parameters. \n",
    "As well as nonseasonal parameters, the SARIMAX model includes seasonal orders denoted by $P$, $D$, $Q$, and $X$. Data patterns or trends are captured in these seasonal orders. It is useful when the residuals of the model exhibit a seasonal trend or pattern that can be explained by an exogenous variable. \n",
    "The SARIMAX model is an extension of the ARIMA model that takes nonseasonal and seasonal factors into account. For modeling time series data that exhibit seasonal patterns or trends, this approach provides a more comprehensive framework. Incorporating seasonal orders enhances the model's ability to capture and explain seasonal dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73306f4534908a86b39f0463e329e41236033cfa"
   },
   "outputs": [],
   "source": [
    "mod = SARIMAX(ts_decompose, trend='n', order=(1,1,0), seasonal_order=(3,0,3,4))\n",
    "resSARIMAX = mod.fit()\n",
    "pred = resSARIMAX.predict()\n",
    "\n",
    "# Raw time serie\n",
    "fig = plt.figure(constrained_layout=True) \n",
    "gs = gridspec.GridSpec(2, 1, figure=fig)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ts_decompose)\n",
    "ax.set_xlabel('time [months]')\n",
    "ax.set_ylabel('data')\n",
    "ax.set_title('Logged data')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, :])\n",
    "ax2.plot(pred)\n",
    "ax2.set_xlabel('time [months]')\n",
    "ax2.set_ylabel('data')\n",
    "ax2.set_title('SARIMAX model')\n",
    "print('SARIMAX RMSE: %.6f'% np.sqrt(sum((pred-ts_decompose)**2)/len(ts)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, a SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous factors) model is fitted to the decomposed time series and predictions are generated. \n",
    "The first step is to initialize a SARIMAX model with specific parameters. The input is the ts_decompose time series. It is set to 'n' to indicate that no trend component is included in the model. For nonseasonal AR, differencing, and MA terms, the order parameter is (1, 1, 0). The `seasonal_order` parameter specifies the seasonal AR, differencing, MA, and the length of the seasonal cycle (3, 0, 3, 4). \n",
    "Following this, the SARIMAX model is fitted to the data using the fit() method, and the resulting results are stored in the resSARIMAX object. \n",
    "Predictions are generated using the `predict()` method and assigned to the variable `pred`. \n",
    "To display the raw time series and predicted values, a figure object is created. Two subplots are arranged using constrained_layout=True and gridspec.GridSpec(2, 1). \n",
    "In the first subplot (`ax`), the decomposed time series (`ts_decompose`) is plotted. On the x-axis are the months, and on the y-axis are the data values. Accordingly, the plot is labeled and titled. \n",
    "In the second subplot (`ax2`), the predicted values (`pred`) are plotted. In this graph, the x-axis represents time in months, and the y-axis represents data values. Labels and titles are provided for the plot. \n",
    "Lastly, the code calculates and prints the root mean square error (RMSE) between the predicted values and the original `ts_decompose` series. SARIMAX's RMSE indicates how well the model fits the data. \n",
    "This code segment fits a SARIMAX model to the decomposed time series, generates predictions, and visualizes the raw data and predictions. By calculating and printing the RMSE, it assesses the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17e5e47fddb122a39de522daa8fb45c0e6f9eeab"
   },
   "source": [
    "In conclusion  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARIMA and SARIMAX models both have their advantages and disadvantages. ARIMA is easier to implement and determine the optimal parameters that fit the data well. Seasonal data, however, are not well suited to it. In contrast, the SARIMAX model captures and models seasonal patterns in the data more effectively. However, it can be difficult to identify the appropriate parameters for the SARIMAX model. \n",
    "The best parameters for SARIMAX models can be selected using various algorithms. ARMA processes can be estimated using Kalman filters, likelihood estimation methods, and the Hyndman-Khandakar algorithm (used in R's automatic ARIMA function). By using these algorithms, parameter selection can be automated and SARIMAX models can be made more accurate. \n",
    "Below is a comparison of the ARIMA and SARIMAX models. Based on the lower root mean squared error (RMSE) of 215.1053, the SARIMAX model provides a better fit to the time series data. SARIMAX captures the underlying patterns and structures in the data better than ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7e8423dd2ecb2e5a91e87d90002c653b9685a48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_SARIMA_diff = pd.Series(pred, copy=True)\n",
    "predictions_SARIMA_diff_cumsum = predictions_SARIMA_diff.cumsum()\n",
    "predictions_SARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\n",
    "predictions_SARIMA_log = predictions_SARIMA_log.add(predictions_SARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_SARIMA = np.exp(predictions_SARIMA_log)\n",
    "\n",
    "predictions_ARIMA_diff = pd.Series(res.fittedvalues, copy=True)\n",
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "predictions_ARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "plt.plot(predictions_ARIMA)\n",
    "print('ARIMA RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)/len(ts)))\n",
    "\n",
    "plt.plot(ts)\n",
    "plt.plot(predictions_SARIMA)\n",
    "print('SARIMA RMSE: %.4f'% np.sqrt(sum((predictions_SARIMA-ts)**2)/len(ts)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ARIMA and SARIMA models, the provided code performs post-processing steps. To evaluate the performance of the models, the predictions are transformed back to their original scale and the root mean squared error (RMSE) is calculated. \n",
    "The code creates a series named predictions_ARIMA_diff which contains the predicted values for the ARIMA model. The cumulative sum of the predicted differences is then calculated and stored in the `predictions_ARIMA_diff_cumsum` series. In addition to these series, there is another series called prediction_ARIMA_log, which is initialized with the logarithmic transformation of the original time series. Predictions_ARIMA_log series are adjusted by adding the cumulative sum of differences (predictions_ARIMA_diff_cumsum), with any missing values filled with 0s. As a final step, the exponential function is applied to the predictions_ARIMA_log series to reverse the logarithmic transformation and obtain the predictions_ARIMA series with the original scale. \n",
    "The SARIMA model predictions are also post-processed in the same way. In the predictions_SARIMA_diff series, the predicted values are stored, and in the predictions_SARIMA_diff_cumsum series, the predicted differences are summed. Predictions_SARIMA_log is initialized to the value of the logarithmic transformation of the original series. Missing values are filled in by adding the cumulative sum of differences. As a final step, the exponential function is used to generate the `predictions_SARIMA` series with predictions in the original scale. \n",
    "The code then calculates the root mean square error (RMSE) between the predictions (`predictions_ARIMA` and `predictions_SARIMA`) and the original time series (`ts`). Model accuracy is measured by RMSE, with a lower value indicating better performance. A printout of the ARIMA model's RMSE is provided. \n",
    "Finally, the code plots both ARIMA and SARIMA predictions against the original data. SARIMA's RMSE is also calculated and printed, providing an evaluation of its performance. \n",
    "Summary: The code segment converts predictions back to their original scale and evaluates the accuracy of ARIMA and SARIMA models using the RMSE metric. For comparison, the predictions are displayed alongside the original time series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
